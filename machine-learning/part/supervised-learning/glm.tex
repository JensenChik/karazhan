% !TEX root = ../../main.tex
\chapter{线性模型} % (fold)
\label{cha:线性模型}

\section{线性回归及其正则} % (fold)
\label{sec:线性回归及其正则}
对于线性回归我们再熟悉不过，然而我们中的很大一部分人，只知道线性回归是利用一条直线（或高维空间下的超平面）去拟合目标值，却对一些细节的知识点知之甚少，比如，线性回归为什么叫线性回归？为什么目标函数选择平方误差？什么情况下线性回归不 work？如果不work又应该如何调整？本小节的目的不在于介绍什么是线性回归，而是对线性回归做一个稍微深入一点的 insight。

回归（Regression）这一个词最初起源于英国高尔顿爵士在人类遗传学的研究\cite{wiki.Regression_toward_the_mean}，他发现一个有趣的现象是，如果父亲的身高高于平均值，那么儿子身高大于父亲身高的概率会低于小于父亲身高的概率，对应的，如果父亲的身高低于平均值，那么儿子身高小于父亲身高的概率会低于大于父亲身高的概率。高尔顿讲这种现象总结为，偏离中心的父母其后代往往更靠近中心，并称之为 Regression，因为Regression有倒退、衰退的含义，所以这个过程描述为{\wei 向着平凡回归}，而回归分析，其目的就是寻找那条平凡的曲线，即，寻找数据背后的脊梁。


\subsection{线性回归} % (fold)
\label{sub:线性回归}
\input{part/supervised-learning/images/boston_house_price}
现在让我们来考虑一个称之为波士顿房价的问题\cite{harrison1978hedonic}，在这个数据集中一共有506个样本，每个样本包含13个特征，分别为
\begin{enumerate}
	\item CRIM：城镇人均犯罪率
    \item ZN：住宅用地超过25000平方英尺的比例
    \item INDUS：城镇非零售商用土地的比例
    \item CHAS：是否位于查理斯河边上
    \item NOX：一氧化氮浓度
    \item RM：住宅平均房间数
	\item AGE：1940 年之前建成的自用房屋比例
	\item DIS：到波士顿五个中心区域的加权距离
	\item RAD：放射性高速公路的接近指数
	\item TAX：每一万美元的全值财产税率
	\item PTRATIO：城镇师生比例
	\item B：$1000(Bk - 0.63)^ 2$，其中 $Bk$ 指代城镇中黑人的比例
	\item LSTAT：人口中地位低下者的比例
\end{enumerate}

为了简单起见，我们只选择其中的第6个变量，即住宅建筑每户人家的平均房间数，作为自变量$x$，对应的因变量$y$为住宅建筑的价格，其散点图如图\ref{fig:boston_house_price}所示。观察数据的走势，大体上呈线性关系\footnote{对于一些异常点，其$y$值统一为$50.0$}，自然而然的你会想到用一个直线来刻画这个数据，即
\begin{equation}
	y = wx + b
\end{equation}

那么机器学习的目的就是利用这506个样本去学习这条曲线的参数，也就是$w$和$b$，学习过程中使用的目标函数为平方误差，即

\begin{equation}\label{equ:loss_of_linear_regress}
	L(w, b) = \Big(y - (wx + b)\Big)^2
\end{equation}



\subsubsection{学习算法} % (fold)
\label{ssub:学习算法}
我们考虑这个问题的解析法，对于式\ref{equ:loss_of_linear_regress}而言，未知的是$w$和$b$两个变量，当只有一个样本的时候，$w$和$b$有无穷多解，更一般地，在线性模型中，当样本量$N$小于模型参数规模$M$时，解有无穷多个\footnote{因为此时是不满秩的状态}，我们称其为欠定方程组；当$N$恰好等于$M$时，模型有唯一解或无解\footnote{取决于是否满秩}，此时称为正定方程组；当$N$大于$M$时，模型无解，此时称为超定方程组。注意一点，我们说无解只是说不存在一组参数使得模型能恰好拟合所有样本，当无解时，我们学习到的参数是近似解。

现在我们有506个样本，而未知的变量只有两个，方程组可以描述为
\begin{equation}\label{eq:欠定方程组}
\begin{bmatrix}
	x_{1} & 1\\
	x_{2} & 1\\
	\vdots & \vdots\\
	x_{506} &  1\\
\end{bmatrix}
\begin{bmatrix}
	w\\
	b\\
\end{bmatrix}
= 
\begin{bmatrix}
	y_{1}\\
	y_{2}\\
	\vdots\\
	y_{506}
\end{bmatrix}
\end{equation}

不妨将式\ref{eq:欠定方程组}简记为
\begin{equation}
	X\theta = Y
\end{equation}

如果$X$是非奇异的，那么我们立刻就能得到解$\theta = X^{-1}Y$，但现在$X$不是一个方阵而是一个超定矩阵，所以不能直接求逆，为此，我们两边同时乘上$X^T$，则得到
\begin{equation}
	X^TX\theta = X^TY
\end{equation}
此时$X^TX$是方阵，那么就能两边乘上它的逆$(X^TX)^{-1}$，得到最终的解
\begin{equation}
	\theta = (X^TX)^{-1}X^TY
\end{equation}
其中，我们称$(X^TX)^{-1}X^T$为$X$的伪逆矩阵

更一般的，$(X^TX)$并不一定可逆，当不可逆时，往往会加上一个微小量的对角矩阵，因此，通用的解为
\begin{equation}\label{equ:linear_regress_approx_solve}
	\theta = \lim_{\epsilon\rightarrow 0}(X^TX + \epsilon I)^{-1}X^TY
\end{equation}


这种解法之所以成立，是因为其背后隐含了平方误差的假设。现在让我们从平方误差的假设出发，重新推导一遍，对于目标函数
\begin{equation}
	L(\theta) = || X\theta - Y||^2 = \sum_{i=1}^{506}(X_i\theta - Y_i)^2
\end{equation}
为了求最小值，我们对$\theta$求导数，得到
\begin{equation}
	\begin{split}
	\frac{d L(\theta)}{d \theta} &= \sum_{i=1}^{506}2(X_i\theta - Y_i)X_i\\
	&= 2X^T(X\theta - Y)
	\end{split}
\end{equation}
当导数为0时取得极值，因此必要条件为
\begin{equation}
X^TX\theta = X^TY
\end{equation}
同样的，我们再一次推出了
\begin{equation}
	\theta = (X^TX)^{-1}X^TY
\end{equation}

上述算法的Python实现如代码\ref{code:boston_house_price_solve1}所式	，其回归结果如图\ref{fig:boston_house_price}中的红线所式
\input{part/supervised-learning/codes/boston_house_price_solve1}

% subsubsection 学习算法 (end)

\subsubsection{残差分析} % (fold)
\label{ssub:残差分析}
现在我们从残差的角度讨论一下为什么目标函数选择平方误差，首先，如图\ref{fig:boston_house_price_3d} 所示，我们假设拟合的残差服从均值为0，方差恒定的高斯分布，即对于
\begin{equation}
	y = \theta x + \epsilon
\end{equation}
其残差$\epsilon$服从
\begin{equation}
\epsilon \sim \mathcal{N}(0, \sigma^2)
\end{equation}

\input{part/supervised-learning/images/boston_house_price_3d}

那么对于每一个样本$(x_i, y_i)$，得到的残差概率为
\begin{equation}
	P(x_i, y_i|\theta) = \frac{1}{\sqrt{2\pi\sigma^2}}
	\exp\Bigg(-\frac{(y_i - \theta x_i)^2}{2\sigma^2}\Bigg)
\end{equation}
常数项对于对数似然最大化没有影响，因此
\begin{equation}
	\log \mathcal{L} \propto  \sum_{i=1}^N - (y_i - \theta x_i)^2
\end{equation}
所以，平方误差作为目标函数是对数似然最大化推导出来的结果，而不是我们拍脑袋选出来的。要最大化对数似然，等价于最小化平方误差。


% subsubsection 残差分析 (end)


\subsubsection{异方差} % (fold)
\label{ssub:异方差}
接下来我们讨论异方差的问题，现在我们的自变量选定为第 13 个变量，即⼈口中地位低下者的⽐例，因变量依然选择房价，那么数据的分布图，以及利用线性回归所得到的曲线如图\ref{fig:boston_house_price2}(a)所式。
\input{part/supervised-learning/images/boston_house_price2}

观察数据的分布不难发现其并不呈线性趋势，若此时强行采用线性回归，则在头部和尾部并不能很好的拟合数据。绘制预测变量与残差的关系如图\ref{fig:boston_house_price2}(b)所示，我们可以看到，随着预测值的增长，残差的方差也在不断的增大，这显然与我们之前的假设，即，残差服从均值为0，方差恒定的高斯分布相矛盾，因为这里的方差是变化的，我们称这种现象为异方差。

像图\ref{fig:boston_house_price2}(b)这种大体上呈现锥形的残差图，其出现的原因是因变量服从乘法模型
\begin{equation}\label{equ:product_model}
	y = (\theta x)\epsilon
\end{equation}
而不是加法模型
\begin{equation}
	y = \theta x + \epsilon
\end{equation}
在这个条件下，残差为
\begin{equation}
	(\theta x)\epsilon - \theta x = \theta x(\epsilon - 1)
\end{equation}
因此，随着预测值$\theta x$的增加，残差也在不断的增加，并且，对于残差的方差
\begin{equation}
	Var\Big((\theta x)\epsilon - \theta x \Big) = (\theta x)^2 Var(\epsilon)
\end{equation}
所以，随着预测值$\theta x$的增加，残差的方差也在呈平方级增长。对于式\ref{equ:product_model}所描述的乘法模型，合适的方法是，不直接回归因变量$y$，而是回归其对数值，通过取对数的方式，便可以将乘法模型转换回加法模型，即
\begin{equation}
	\log y = \log (\theta x)\epsilon = \log\theta x + \log \epsilon
\end{equation}

\input{part/supervised-learning/images/boston_log_res}

如图\ref{fig:boston_house_price}所示是回归$\log y$后的残差图，我们可以看到，对数变换使得残差的方差变得稳定，锥形得到抑制，我们认为对数模型比没有采用变换的模型其回归结果更可靠。

对于抛物线形状的残差图，是由于数据呈泊松分布造成的，此时的应变量往往是每个单位（时间、距离、体积等）内的计数，应该回归$\sqrt{y}$；对于椭球形状的残差图，是由于数据呈二项分布造成的，此时的应变量往往是百分数、占比等概率值，或者$n$次实验中成功的次数占比，应该回归$\sin^{-1}\sqrt{y}$；对于呈锥形的残差图，是由于误差服从乘法模型，此时的应变量往往是商业、人文、经济类型的数据，应该回归$\ln y$。

% subsubsection 异方差 (end)


\subsubsection{共线性} % (fold)
\label{ssub:共线性}
现在我们把自变量扩展到多个，并考虑其共线性问题，为此，我们选择第6、9、10个变量，即住宅平均房间数、放射性⾼速公路的接近指数、每⼀万美元的全值财产税率作为自变量，房价作为因变量，进行多重线性回归。为了描述共线性的问题，我们每次只随机选取506个样本中的一部分样本进行学习，然后测量多次回归后，参数的方差，这个过程如代码\ref{code:boston_house_price_co_linear} 所示。
\input{part/supervised-learning/codes/co-learner}

可以看出，对于各个自变量与偏置，多次回归得到的参数估计其方差为
\begin{equation}
	[\sigma_6,\sigma_9,\sigma_{10},\sigma_b] \simeq [0.679,0.069,0.003,4.745]
\end{equation}

观察一下变量9，其方差约为0.069，若我们将第10个变量更换为第8个变量，即到波⼠顿五个中⼼区域的加权距离，现在我们选择第6、9、8个变量作为自变量，因变量依然选择房价，与上述同样的操作，多次回归后得到的参数估计方差为
\begin{equation}
	[\sigma_6,\sigma_9,\sigma_8,\sigma_b] \simeq [ 0.665,0.042,0.108,4.086]
\end{equation}

\input{part/supervised-learning/images/var_linear_relation}

再次观察一下变量9，其方差现在约为0.042，大约降低了$50\%$左右，而变量6和偏置的方差没有显著的变化。之所以出现这种现象的原因在于，第一次实验中的三个变量，变量9和变量10是线性相关的，我们可以绘制其关系如图\ref{fig:var_linear_relation}(a)所示。而第二次实验中，我们把与变量9相关的变量10剔除掉，替换成与变量9不太相关的变量8，使得变量9的方差显著降低。为了验证我们这一说辞，我们不妨再做一次实验，将变量8替换成变量9加上一个区间在0到0.1的随机均匀噪声，即，现在的三个自变量为：变量6、变量9、带微小噪声的变量9，重复执行一次上述过程，将得到
\begin{equation}
	[\sigma_6,\sigma_9,\sigma_{9^*},\sigma_b] \simeq [ 0.656, 9.270, 9.276, 4.110]
\end{equation}

由于现在的第三个因变量与第二个因变量是高度相关的，所以可以看到，方差显著地增长到了9.27。这种因为变量之间存在相关性而使得参数估计的方差变大的现象，我们成之为共线性问题。

接下来我们分析一下为什么会产生这种现象。由于平方误差假设下的最优参数估计为
\begin{equation}
\begin{split}
	\hat{\theta} &= (X^TX)^{-1}X^TY \\ 
	&= (X^TX)^{-1}X^T(\theta X + \epsilon)\\
	&= \theta + (X^TX)^{-1}X^T\epsilon
\end{split}
\end{equation}

并且，$\hat{\theta}$是对$\theta$的无偏估计，即
\begin{equation}
\begin{split}
	E(\hat{\theta}) &= E\big[\theta + (X^TX)^{-1}X^T\epsilon\big] \\
	&=\theta + (X^TX)^{-1}X^TE(\epsilon) \\
	&=\theta
\end{split}
\end{equation}
那么对于参数估计的协方差，有
\begin{equation}
\begin{split}
Cov(\hat{\theta}, \hat{\theta}) &= E\Big[\big(\hat{\theta} - E(\hat{\theta})\big)\big(\hat{\theta} - E(\hat{\theta})\big)^T\Big] \\
&= E\Big[(\hat{\theta} - \theta)(\hat{\theta} - \theta)^T\Big] \\
&= E\Big[\big((X^TX)^{−1}X^T\epsilon\big)\big((X^TX)^{−1}X^T\epsilon\big)^T\Big] \\
&= E\Big[(X^TX)^{−1}X^T\epsilon\epsilon^TX(XX^T)^{−1}\Big] \\
&= (X^TX)^{−1}X^TE(\epsilon\epsilon^T)X(XX^T)^{−1}\\
&= (X^TX)^{−1}X^T\sigma^2 IX(XX^T)^{−1}\\
&= \sigma^2 I(X^TX)^{−1}X^TX(XX^T)^{−1}\\
&= \sigma^2 (XX^T)^{−1}\\
\end{split}
\end{equation}
其中$\sigma^2$是残差的方差。因此，对于某一个自变量的参数$\theta_i$，其方差为$\sigma^2(XX^T)^{−1}$在对角线上的第$i$个元素。

接下来，为了简单起见，我们通过所有自变量的方差之和来衡量整体方差
\begin{equation}
	tr\big[\sigma^2(XX^T)^{−1}\big] = \sigma^2\sum_i\frac{1}{\lambda_i}
\end{equation}
其中 $\lambda_i$是矩阵$XX^T$的特征值。当变量间的共线性越强，行列式$XX^T$将去向于0，必定会存在某个$\lambda$趋向于0，从而导致$tr\big[\sigma^2(XX^T)^{−1}\big]$趋向于无穷。这也就是为什么共线性会导致参数估计的方差变大的原因。

我们从直观上在来感受一下共线性的问题，假设使用两个变量$x_1$，$x_2$进行拟合，$x_2$的最优估计为$\theta_2 = 0.5$，当使用三个变量$x_1$，$x_2$，$x_2$ 来拟合时，由于$x_2$是共线性的，所以只需要满足$\theta_2 + \theta_3 = 0.5$就能达到同样的性能，这时候单独对于$\theta_2$和$\theta_3$而言有很高的自由度，所以方差也就增大了。

尽管共线性会导致参数估计的方差过高，但这依然是一个无偏的估计，所以共线性在训练集上可能拟合得很好，但在测试集上就不一定，因为过高的方差抑制了模型的泛化能力。

% subsubsection 共线性 (end)

% subsection 线性回归 ( end)


\subsection{Ridge 回归} % (fold)
\label{sub:ridge_回归}
接下来我们讨论一种称之为岭回归（Ridge Regression）的方法，这种方法大体上于线性回归类似，唯一不同的地方在于，目标函数加上了权值的二范数正则，即
\begin{equation}\label{equ:mse_l2_loss}
	\begin{split}
		L(\theta) &= (X\theta  - Y)^2 + \alpha \sum_i\theta_i^2\\
		 &= (X\theta - Y)^2 + \alpha \theta\theta^T
	\end{split}
\end{equation}
如果从拉格朗日乘子法的角度来看这个问题，那么这个目标函数实际上等价于带有不等式约束的极值问题，即
\begin{equation}
\begin{split}
	&L(\theta) = (X\theta - Y)^2 \\
    &s.t.\text{~} \sum_i\theta_i^2 \leq C
\end{split}
\end{equation}
那么式\ref{equ:mse_l2_loss}实际上近似于构造出来的拉格朗日函数\footnote{无关项被省略或合并了，所以不完全等价于拉格朗日函数}。根据拉格朗日乘子法的套路，目标函数\ref{equ:mse_l2_loss}的极值等于函数\footnote{比如这里的平方误差}在可行域\footnote{比如这里的超球体}上的极值，一个二维空间下的形象描述如图\ref{fig:L2_lang}所示。

\input{part/supervised-learning/images/L2_lang}

对于图\ref{fig:L2_lang}(a)中最优解落在可行域的情况，二范数正则并不影响参数估计的结果，此时参数估计依然是无偏的。但实际场景中往往是图\ref{fig:L2_lang}(b)中的情况，即最优解落在可行之外的情况，此时参数估计是有偏的，并且，由于约束条件下的极值为目标函数于可行域的切面，而可行域是以原点为球心，以微小量为半径的超球体，这将使得参数最终的估计值是一个较小的值，为了进一步说明这个现象，我们不妨对式\ref{equ:mse_l2_loss}中的准则函数进行求导
\begin{equation}
\begin{split}
	\frac{\partial L(\theta)}{\partial \theta} &= \frac{\partial(X\theta  - Y)^2}{\partial \theta} + 2\alpha \theta\\
	&= \nabla_{MSE} + 2\alpha \theta
\end{split}
\end{equation}
当使用梯度下降进行训练是，参数的更新方式为
\begin{equation}
	\theta := \theta -\eta (\nabla_{\theta} + 2\alpha \theta)
\end{equation}
其中,$\eta$是学习率，由于$\alpha$是超参数，当其选取为
\begin{equation}
	\alpha = \frac{1 - \eta}{2\eta}
\end{equation}
则有
\begin{equation}
\begin{split}
	\theta &:= \theta -\eta (\nabla_{\theta} + \frac{1-\eta}{\eta} \theta)\\
	&= \theta - \eta\nabla_{\theta} - (1-\eta)\theta\\
	&=\eta(\theta - \nabla_{\theta})
 \end{split}
\end{equation}
这其实等价于两个参数更新过程，首先执行梯度更新，即$\theta - \nabla_{\theta}$，然后再将更新后的参数按$\eta$进行缩小，这种技术也被称为权衰减，所以我们说，权衰减等价于L2正则。

如果从罚函数的观点看L2正则，那么对于罚函数$\theta\theta^T$而言，当$\theta$较大时，惩罚项也较大，当$\theta$趋于0时，惩罚项趋于0，但此时梯度$\theta$也相应地趋于0，因此$\nabla_{\theta}$在这时候成为主导地位，此时$\theta$有可能会增大，所以$\theta$很难收敛到0。


接下来我们讨论一下为什么加了L2正则的线性回归被称为岭回归。对于目标函数\ref{equ:mse_l2_loss}，其极值所在点为导数为0处，即
\begin{equation}
	2X^T(\theta X - Y) + 2\alpha\theta = 0
\end{equation}
则
\begin{equation}\label{equ:L2_theta}
	\theta = (X^TX + \alpha I)^{-1}X^TY
\end{equation}
可以看到，这个结果于式\ref{equ:linear_regress_approx_solve}是等价的。由于$X^TX + \alpha I$存在一个微小量$\alpha I$，所以它的逆是必定存在的，不需要考虑不可逆的问题，同时，正是因为在$X^TX$的对角线上加了这个微小量，相当于增高了它的“岭”，所以称为岭回归或脊回归。 

\input{part/supervised-learning/images/L2_ridge_trace}

我们可以绘制绘制出岭迹线如图\ref{fig:L2_ridge_trace}所示。其中，自变量我们选取的是住宅平均房间数，因变量选取的是房价。图\ref{fig:L2_ridge_trace}(b)中的$x$轴是岭的增幅，也就是L2正则的强度，$y$轴是偏置项的估计值，我们可以看到，随着正则的强度加大，偏置项的估计值逐步降低，而对应的，图\ref{fig:L2_ridge_trace}(a)表明，随着正则的强度加大，自变量的参数估计值反而逐步增大，之所以出现这种现象是因为自变量的参数估计本身处于一个较小的值（0.35）左右，而偏执项的估计处于较大的值（0.71）左右，L2正则会衰减较大的值，也就是偏置项的参数估计，对于较小的值，也就是自变量的参数估计，L2的地位低于误差函数的地位，所以为了降低误差，自变量的参数估计便会升高，最后会得到相对平滑的参数估计。为了更清晰地说明这一点，我们需要从贝叶斯的角度来考虑L2正则这个问题。

在贝叶斯的视角下，现在，模型的参数$\theta$不再认为是客观存在但未知的变量，而是一个随机变量，这也是贝叶斯学派和经典学派的分歧点，即，模型的参数是客观存在的，还是一个随机变量。我们假设，$\theta$服从一个均值为0，方差恒定的高斯分布，即
\begin{equation}
	\theta \sim \mathcal{N}(0, \sigma_\theta^2)
\end{equation}
并且，与线性回归中讨论的类似，我们依然假设回归的残差服从均值为0，方差恒定的高斯分布
\begin{equation}
	\epsilon \sim \mathcal{N}(0, \sigma_\epsilon^2)
\end{equation}
那么对于$\theta$的后验概率，有
\begin{equation}
	p(\theta|x, y) = \frac{p(x, y| \theta)p(\theta)}{p(x, y)} 
	= \frac{(2\pi\sigma_\epsilon^2)^{-\frac{1}{2}}\exp\Big(-\frac{(X\theta - Y)^2}{2\sigma_\epsilon^2}\Big)
	(2\pi\sigma_\theta^2)^{-\frac{1}{2}}\exp\Big(-\frac{\theta^T\theta}{2\sigma_\epsilon^2}\Big)
	}{P(x, y)}
\end{equation}
由于分母为归一化因子\footnote{为了保证概率第三公理成立}，所以是一个常数项，此外，其他的常数项对于最大化后验概率没有影响，因此
\begin{equation}
	p(\theta|x, y) \propto
	\exp\Big(-\frac{(X\theta - Y)^2}{2\sigma_\epsilon^2}\Big)
	\exp\Big(-\frac{\theta^T\theta}{2\sigma_\epsilon^2}\Big)
\end{equation}
由于这是一个单调函数，对其取对数变换不影响极值的解，因此
\begin{equation}
	\log p(\theta|x, y) \propto
	-(X\theta - Y)^2
	-\theta^T\theta
\end{equation}
因此，为了最大化后验概率，实际上等价于最小化带L2正则的平方误差。

\input{part/supervised-learning/images/L2_hist}


另一方面，正如我们在图\ref{fig:L2_hist}中看到的，加入了L2范数正则后，参数估计不再保证无偏性，因为此时

\begin{equation}\label{equ:L2_theta_expect}
\begin{split}
E(\hat{\theta}) &=E\big[(X^TX + \alpha I)^{-1}X^TY\big]\\
&=E\big[(X^TX + \alpha I)^{-1}X^T(X\theta + \epsilon)\big]\\
&=(X^TX + \alpha I)^{-1}X^TX\theta 
	+  (X^TX + \alpha I)^{-1}X^T E\big(\epsilon)\\
&=(X^TX + \alpha I)^{-1}X^TX\theta 
 \end{split}
\end{equation}

因此，L2正则相当于对参数估计引入了偏差，但同时也带来了它的好处：降低了参数估计的方差。为了更严谨地证明这一点，我们首先需要引入奇异值分解的概念。对于任何一个矩阵 $X\in \mathbf{R}^{m\times n}$，都能分解成三个矩阵相乘
\begin{equation}
	X = U\Sigma V^T
\end{equation}
其中，$U \in \mathbf{R}^{m\times m}$和$V \in \mathbf{R}^{n\times n}$是正交矩阵\footnote{如果在酉空间下则为$X = U\Sigma V^H$，$U$和$V$此时为酉矩阵}，即满足$UU^T = I$ 和 $VV^T = I$，并且$\Sigma \in \mathbf{R}^{m\times n} = \begin{bmatrix}
	\Sigma_1 & O\\
	O &  O\\
\end{bmatrix}
$满足 $\Sigma_1 = diag(\lambda_1, \lambda_2, \cdots, \lambda_r)$，$r = rank(X)$，并且对于任意的 $i > j$，$\lambda_i \geq \lambda_j > 0$

对于$\hat{\theta}$参数估计的协方差，有
\begin{equation}
	cov(\hat{\theta}, \hat{\theta}) = E\Big[\big(\hat{\theta} - E(\hat{\theta})\big)\big(\hat{\theta} - E(\hat{\theta})\big)^T\Big]\\
\end{equation}
代入\ref{equ:L2_theta_expect}，有
\begin{equation}
	cov(\hat{\theta}, \hat{\theta}) = 
	E\Big[\big(\hat{\theta} - (X^TX + \alpha I)^{-1}X^TX\theta \big) 
	\big(\hat{\theta} - (X^TX + \alpha I)^{-1}X^TX\theta \big)^T\Big]
\end{equation}
由于
\begin{equation}
\begin{split}
	\hat{\theta} - (X^TX + \alpha I)^{-1}X^TX\theta &= 
	(X^TX + \alpha I)^{-1}X^TY - (X^TX + \alpha I)^{-1}X^TX\theta\\
	&= (X^TX + \alpha I)^{-1}X^T(X\theta + \epsilon) - (X^TX + \alpha I)^{-1}X^TX\theta\\
	&= (X^TX + \alpha I)^{-1}X^T\epsilon
\end{split}
\end{equation}
因此
\begin{equation}
cov(\hat{\theta}, \hat{\theta})	
= E\Big[
(X^TX + \alpha I)^{-1}X^T\epsilon
\epsilon^T X (X^TX + \alpha I)^{-T}\footnote{由于$(A^{-1})^T = (A^{T})^{-1}$，因此我们简写为$A^{-T}$}
\Big]
\end{equation}
再整理整理，最后我们得到
\begin{equation}
	cov(\hat{\theta}, \hat{\theta}) = \sigma^2(X^TX + \alpha I)^{-1}X^T
	X (X^TX + \alpha I)^{-T}
\end{equation}
其中，$\sigma^2$是$\theta$残差的方差。接下来我们对$X$进行SVD分解，则
\begin{equation}
\begin{split}
	cov(\hat{\theta}, \hat{\theta}) &= \sigma^2
	(V\Sigma^TU^TU\Sigma V^T + \alpha VV^T)^{-1}
	V\Sigma^TU^TU\Sigma V^T
	(V\Sigma^TU^TU\Sigma V^T + \alpha VV^T)^{-T}\\
	&= \sigma^2
	\big(V(\Sigma^2+\alpha I)V^T\big)^{-1}
	V\Sigma^2V^T
	\big(V(\Sigma^2+\alpha I)V^T\big)^{-T}\\
	&=\sigma^2
	V^{-T}(\Sigma^2+\alpha I)^{-1}V^{-1}
	V\Sigma^2V^T
	V^{-T}(\Sigma^2 + \alpha I)^{-T}V^{-1}\\
	&=\sigma^2
	V^{-T}(\Sigma^2 + \alpha I)^{-1}
	\Sigma^2
	(\Sigma^2 + \alpha I)^{-T}V^{-1}\\
\end{split}
\end{equation}
同样的，我们以协方差的迹衡量参数估计的方差，并且由于迹的性质
\begin{equation}
	tr(ABC) = tr(BCA)
\end{equation}
因此岭回归参数估计的方差为
\begin{equation}
\begin{split}
	tr\big(cov(\hat{\theta}, \hat{\theta})\big) &= \sigma^2 
	tr\big(
	(\Sigma^2 + \alpha I)^{-1}
	\Sigma^2
	(\Sigma^2 + \alpha I)^{-T}
	\big) \\ 
	&= \sigma^2 tr
	\begin{bmatrix}
	\frac{\lambda_i^2}{(\lambda_i^2 + \alpha)^2} & O \\
	O  &  O
	\end{bmatrix}\\
	&= \sigma^2 \sum_{i=0}^r \frac{\lambda_i^2}{(\lambda_i^2 + \alpha)^2}
\end{split}
\end{equation}
其中$r = rank(X)$。同样的处理，对于线性回归中参数估计的协方差矩阵的迹
\begin{equation}
\begin{split}
	tr\big(cov(\hat{\theta}, \hat{\theta})\big) &= \sigma^2tr(X^TX)^{-T} = \sigma^2 tr(V\Sigma^TU^T U\Sigma V^T)^{-T}\\
	&= \sigma^2 tr\big(V^{-T}(\Sigma^2)^{-T} V^{-1}\big) \\
	&= \sigma^2 tr \begin{bmatrix}
	\frac{1}{\lambda_i^2} & O \\
	O & O \\
	\end{bmatrix} = \sigma^2 \sum_{i=0}^r \frac{1}{\lambda_i^2}
\end{split}
\end{equation}
对于任意的$\lambda > 0$，$\alpha > 0$，不等式
$\frac{\lambda^2}{(\lambda^2 + \alpha)^ 2} < \frac{1}{\lambda^2}$
恒成立，所以岭回归可以降低参数估计的方差。




% subsection ridge_回归 (end)






\subsection{Lasso 回归} % (fold)
\label{sub:lasso_回归}
在这一小节中，我们将会介绍与Ridge回归类似的正则化技术，称之为Lasso（Least absolute shrinkage and selection operator）回归\footnote{中文有时称之为套索回归，源自单词Lasso的释义}。在Lasso回归中，目标函数不再使用L2正则，取而代之的是L1正则，即
\begin{equation}
	L(\theta) = (X\theta -Y)^2 + \alpha ||\theta||
\end{equation}
在拉格朗日乘子法的视角下，这个目标函数相当于优化带有约束条件的极值问题，即
\begin{equation}
	\begin{split}
		&L(\theta) = (X\theta -Y)^2 \\
		&s.t. ||\theta|| \leq C
	\end{split}
\end{equation}
与Ridge回归类似的，我们可以绘制出可行域于目标函数的等势线如图todo所示。从图中我们可以看到，与L2正则收敛到平滑解所不同的是，L1正则更倾向于收敛到稀疏解，这是因为对于带有L1正则的目标函数，其梯度为
\begin{equation}\label{equ:L1_grad}
	\frac{\partial L(\theta)}{\partial \theta} = 2X^T(X\theta - Y) + \alpha sign(\theta)
\end{equation}
其中，$sign$为符号函数
\begin{equation}
	sign(x) = \left\{\begin{array}{cc}
	1 &x > 0\\
	0 &x = 0\\
	-1 & x < 0\\
	\end{array}
	\right.
\end{equation}
可以看到，在L1正则中，由于符号函数的存在，即使参数$\theta$处于一个较小的值，正则化强度依然为$\alpha$，正则化方向的梯度不会消失，而相对应的，在L2正则中，当参数$\theta$处于较小值时，正则化强度随着$\theta$的减小而成平方级的减小，所以相比于L2正则，L1正则能驱动参数一直往0收缩，最后得到稀疏的解，即大部分参数为0，小部分参数不为0。同时，也由于导数中引入了符号函数，所以$\theta$的参数估计没有解析解，只有数值解，因为这是一个多解问题，只要满足式\ref{equ:L1_grad}的$\theta$都是最优解。

与L2正则类似的，L1正则也牺牲了参数估计的无偏性，因为目标函数\ref{equ:L1_grad}等于0处的参数估计
\begin{equation}
	X^TX\hat{\theta}+\frac{\alpha}{2}sign(\hat{\theta}) = X^TY = X^T(X\theta + \epsilon)
\end{equation}
有
\begin{equation}
	X^TXE(\hat{\theta})+\frac{\alpha}{2} E[sign(\hat{\theta})] = X^TX\theta
\end{equation}
假设这是一个无偏估计，则满足$E(\hat{\theta})=\theta$，那么推出
\begin{equation}
	E[sign(\hat{\theta})] = 0 
\end{equation}
这在实际问题中并不一定成立，与无偏估计的假设矛盾。
	

% subsection lasso_回归 (end)



\subsection{ElasticNet} % (fold)
\label{sub:elasticnet}

% subsection elasticnet (end)

\subsection{多项式回归} % (fold)
\label{sub:多项式回归}

% subsection 多项式回归 (end)

% section 线性回归及其正则 (end)

\section{指数族与广义线性模型} % (fold)
\label{sec:指数族与广义线性模型}
\subsection{指数族} % (fold)
\label{sub:指数族}

% subsection 指数族 (end)
\subsection{对数几率回归} % (fold)
\label{sub:对数几率回归}

% subsection 对数几率回归 (end)

\subsection{Softmax 回归} % (fold)
\label{sub:softmax_回归}

% subsection softmax_回归 (end)

\subsection{泊松回归} % (fold)
\label{sub:泊松回归}

% subsection 泊松回归 (end)

% section 指数族与广义线性模型 (end)

\section{贝叶斯方法} % (fold)
\label{sec:贝叶斯方法}

\subsection{贝叶斯回归} % (fold)
\label{sub:贝叶斯回归}

% subsection 贝叶斯回归 (end)

\subsection{朴素贝叶斯} % (fold)
\label{sub:朴素贝叶斯}

% subsection 朴素贝叶斯 (end)


% section 贝叶斯方法 (end)



\section{支撑向量机} % (fold)
\label{sec:支撑向量机}

% section 支撑向量机 (end)


% chapter 线性模型 (end)